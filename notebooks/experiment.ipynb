{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils.perspective import four_point_transform\n",
    "from skimage.filters import threshold_local\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import imutils\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIFACT_PATH = os.path.join(os.path.abspath(\"..\"),\"artifacts\")\n",
    "models_path = os.path.join(ARTIFACT_PATH,\"models\")\n",
    "results_path = os.path.join(ARTIFACT_PATH,\"results\",\"License_Plate\")\n",
    "samples_path = os.path.join(ARTIFACT_PATH,\"samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Edge Detection\n"
     ]
    }
   ],
   "source": [
    "image = cv.imread(os.path.join(results_path,\"plate0.jpg\"))\n",
    "ratio = image.shape[0]/image.shape[0]\n",
    "orig = image.copy()\n",
    "# image = imutils.resize(image, height=300)\n",
    "\n",
    "gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "gray = cv.GaussianBlur(gray, (7,7), 0)\n",
    "edged = cv.Canny(gray, 150, 255)\n",
    "\n",
    "print(\"STEP 1: Edge Detection\")\n",
    "cv.imshow(\"Image\", image)\n",
    "cv.imshow(\"Edged\", edged)\n",
    "\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2: Find contours of paper\n"
     ]
    }
   ],
   "source": [
    "cnts = cv.findContours(edged.copy(), cv.RETR_LIST, cv.CHAIN_APPROX_SIMPLE)\n",
    "cnts = imutils.grab_contours(cnts)\n",
    "cnts = sorted(cnts, key = cv.contourArea, reverse = True)[:5]\n",
    "# loop over the contours\n",
    "for c in cnts:\n",
    "\t# approximate the contour\n",
    "\tperi = cv.arcLength(c, True)\n",
    "\tapprox = cv.approxPolyDP(c, 0.02 * peri, True)\n",
    "\t# if our approximated contour has four points, then we\n",
    "\t# can assume that we have found our screen\n",
    "\tif len(approx) == 4:\n",
    "\t\tscreenCnt = approx\n",
    "\t\tbreak\n",
    "# show the contour (outline) of the piece of paper\n",
    "print(\"STEP 2: Find contours of paper\")\n",
    "cv.drawContours(image, [screenCnt], -1, (0, 255, 0), 1)\n",
    "cv.imshow(\"Outline\", image)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3: Apply perspective transform\n"
     ]
    }
   ],
   "source": [
    "# apply the four point transform to obtain a top-down\n",
    "# view of the original image\n",
    "warped = four_point_transform(orig, screenCnt.reshape(4, 2) * ratio)\n",
    "# convert the warped image to grayscale, then threshold it\n",
    "# to give it that 'black and white' paper effect\n",
    "warped = cv.cvtColor(warped, cv.COLOR_BGR2GRAY)\n",
    "# T = threshold_local(warped, 11, offset = 10, method = \"gaussian\")\n",
    "# warped = (warped > T).astype(\"uint8\") * 255\n",
    "# show the original and scanned images\n",
    "print(\"STEP 3: Apply perspective transform\")\n",
    "cv.imshow(\"Original\", orig)\n",
    "cv.imshow(\"Scanned\", warped)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "# save_path = os.path.join(ARTIFACT_PATH,\"results\",\"cropped\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 4: Apply Histogram equalization\n"
     ]
    }
   ],
   "source": [
    "# Apply histogram equalization\n",
    "equalized_image = cv.equalizeHist(warped)\n",
    "\n",
    "print(\"STEP 4: Apply Histogram equalization\")\n",
    "cv.imshow(\"Scanned\", equalized_image)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "height, width = equalized_image.shape[:2]\n",
    "clip_rows = int(0.25 * height)\n",
    "\n",
    "# Clip the top and bottom rows\n",
    "clipped_image = equalized_image[clip_rows:-clip_rows, :]\n",
    "clipped_image_no_hist = warped[clip_rows:-clip_rows, :]\n",
    "\n",
    "# Display or save the clipped image\n",
    "cv.imshow('Clipped Image', clipped_image)\n",
    "# cv.imshow('Clipped Image', clipped_image)\n",
    "cv.imshow('Clipped Image no hist', clipped_image_no_hist)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "cv.imwrite(os.path.join(ARTIFACT_PATH,\"results\", \"clipped.jpg\"),clipped_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 155)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clipped_image = imutils.resize(clipped_image, height = 50)\n",
    "clipped_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "blurred = cv.GaussianBlur(clipped_image, (9,9), 0)\n",
    "thresh = cv.adaptiveThreshold(clipped_image, 255, cv.ADAPTIVE_THRESH_MEAN_C, cv.THRESH_BINARY_INV, 29, 55)\n",
    "\n",
    "threshold_value = 90\n",
    "max_value = 255\n",
    "# ret, thresh = cv.threshold(blurred, threshold_value, max_value, cv.THRESH_BINARY_INV)\n",
    "\n",
    "cv.imshow(\"test\", imutils.resize(thresh, height = 50))\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, labels = cv.connectedComponents(thresh)\n",
    "mask = np.zeros(thresh.shape, dtype=\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "901\n"
     ]
    }
   ],
   "source": [
    "total_pixels = clipped_image.shape[0] * clipped_image.shape[1]\n",
    "lower = total_pixels // 50\n",
    "upper = total_pixels // 10\n",
    "print(total_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "for(i, label) in enumerate(np.unique(labels)):\n",
    "\n",
    "  if label == 0:\n",
    "    continue\n",
    "\n",
    "  labelMask = np.zeros(thresh.shape, dtype=\"uint8\")\n",
    "  labelMask[labels == label] = 255\n",
    "  numPixels = cv.countNonZero(labelMask)\n",
    "\n",
    "\n",
    "  if numPixels > lower and numPixels < upper :\n",
    "    mask = cv.add(mask, labelMask)\n",
    "\n",
    "cv.imshow(\"test\", imutils.resize(mask, height = 50))\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnts,_ = cv.findContours(mask.copy(), cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "boundingBoxes = [cv.boundingRect (c) for c in cnts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(34, 12, 2, 3),\n",
       " (30, 3, 3, 14),\n",
       " (23, 3, 5, 13),\n",
       " (14, 3, 9, 13),\n",
       " (8, 3, 5, 13),\n",
       " (2, 3, 5, 13),\n",
       " (46, 2, 6, 13),\n",
       " (41, 2, 5, 13),\n",
       " (35, 2, 5, 13),\n",
       " (38, 0, 4, 1)]"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boundingBoxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def compare(rect1, rect2):\n",
    "  if abs(rect1[1] - rect2[1]) > 10:\n",
    "    return rect1[1] - rect2[1]\n",
    "  else:\n",
    "    return rect1[0] - rect2[0]\n",
    "boundingBoxes = sorted(boundingBoxes, key=functools.cmp_to_key(compare))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bbox in boundingBoxes:\n",
    "    x, y, w, h = bbox\n",
    "    cv.rectangle(clipped_image, (x, y), (x + w, y + h), (0, 255, 0), 1)\n",
    "\n",
    "cv.imshow(\"test\", imutils.resize(clipped_image, height = 50))\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 5, 13)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb = boundingBoxes[0]\n",
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb[0]+bb[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop = clipped_image[bb[1]:(bb[1]+bb[3]),bb[0]:(bb[0]+bb[2])]\n",
    "\n",
    "cv.imshow(\"test\", imutils.resize(crop, height = 50))\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anprenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
